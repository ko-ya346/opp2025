from pathlib import Path

import os
import sys
import json
import argparse

import pandas as pd
import numpy as np

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error
from sqlalchemy.engine import create

abspath = Path("/home/kouya-takahashi/kaggle/opp2025/")
sys.path.append(str(abspath))

from src.data import load_data
from src.utils import NULL_FOR_SUBMISSION, score
from src.utils.upload_kaggle_dataset import create_kaggle_dataset_metadata, upload_kaggle_dataset

# --- è¿½åŠ : ãƒ¢ãƒ‡ãƒ«é¸æŠãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ -----------------------------------------
from collections import OrderedDict
from sklearn.linear_model import Ridge

def per_model_mae_table(X: pd.DataFrame, y: np.ndarray) -> pd.Series:
    """
    å„ãƒ¢ãƒ‡ãƒ«åˆ—ã”ã¨ã«ã€(y, ãã®ãƒ¢ãƒ‡ãƒ«åˆ—) ãŒã¨ã‚‚ã«éNaNãªè¡Œã§ MAE ã‚’è¨ˆç®—
    """
    maes = OrderedDict()
    for col in X.columns:
        m = (~np.isnan(y)) & (~X[col].isna().values)
        if m.sum() == 0:
            maes[col] = np.inf
        else:
            maes[col] = mean_absolute_error(y[m], X.loc[m, col].values)
    return pd.Series(maes).sort_values()

def select_models_by_mae(mae_s: pd.Series, rel_delta: float = 0.02, abs_eps: float | None = None,
                         min_keep: int = 1, max_keep: int | None = None) -> list[str]:
    """
    æœ€è‰¯MAEã«è¿‘ã„ãƒ¢ãƒ‡ãƒ«ã®ã¿æ¡ç”¨
      - rel_delta: ç›¸å¯¾é–¾å€¤ï¼ˆbest * (1 + rel_delta) ã¾ã§è¨±å®¹ï¼‰
      - abs_eps:   çµ¶å¯¾å·®ã®é–¾å€¤ï¼ˆbest + abs_eps ã¾ã§è¨±å®¹ã€æŒ‡å®šæ™‚ã¯ä¸¡æ–¹æº€ãŸã™ï¼‰
    """
    if len(mae_s) == 0 or not np.isfinite(mae_s.iloc[0]):
        return []
    best = mae_s.iloc[0]
    keep = []
    for name, v in mae_s.items():
        ok_rel = (v <= best * (1.0 + rel_delta))
        ok_abs = True if abs_eps is None else (v <= best + abs_eps)
        if ok_rel and ok_abs:
            keep.append(name)

    if len(keep) < min_keep:
        keep = [mae_s.index[0]]
    if max_keep is not None and len(keep) > max_keep:
        keep = keep[:max_keep]
    return keep
    
def as_nan(x):
    # æ•°å€¤ã§ã€ç•ªå…µå€¤ã¨ä¸€è‡´ã™ã‚‹ã‚‚ã®ã‚’ NaN ã«
    if pd.isna(x):
        return np.nan
    try:
        return np.nan if float(x) == float(NULL_FOR_SUBMISSION) else x
    except Exception:
        return x

def prepare_oof_table(train, dfs, target):
    """
    train: å­¦ç¿’ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆid, target ã‚’å«ã‚€ï¼‰
    dfs:   {model_name: oof_df (id, target ã‚’å«ã‚€)} ã®è¾æ›¸
    target: ä¾‹ 'Tg'
    æˆ»ã‚Šå€¤: df (åˆ—: ['id','y_true', <model1>, <model2>, ...])
    """
    base = train[['id', target]].dropna().rename(columns={target: 'y_true'}).copy()
    for name, df in dfs.items():
        # id ã§ inner mergeï¼ˆidé †ã‚’ base ã«åˆã‚ã›ã¦å›ºå®šï¼‰
        base = base.merge(df[['id', target]].rename(columns={target: name}),
                          on='id', how='left')
    return base

def rank_average(df_models: pd.DataFrame) -> np.ndarray:
    Z = df_models.loc[:, df_models.notna().any(axis=0)]
    if Z.shape[1] == 0:
        return np.full(len(df_models), np.nan)
    R = Z.rank(method='average', na_option='keep', pct=True)
    return R.mean(axis=1, skipna=True).to_numpy()

def mean_blend(df_models: pd.DataFrame) -> np.ndarray:
    Z = df_models.loc[:, df_models.notna().any(axis=0)]
    if Z.shape[1] == 0:
        return np.full(len(df_models), np.nan)
    return Z.mean(axis=1, skipna=True).to_numpy()

def median_blend(df_models: pd.DataFrame) -> np.ndarray:
    Z = df_models.loc[:, df_models.notna().any(axis=0)]
    if Z.shape[1] == 0:
        return np.full(len(df_models), np.nan)
    return Z.median(axis=1, skipna=True).to_numpy()

def nnls_blend(df_models: pd.DataFrame, y: np.ndarray):
    Z = df_models.loc[:, df_models.notna().any(axis=0)]
    M = Z.columns.tolist()
    if len(M) == 0:
        return np.full(len(df_models), np.nan), np.array([])

    # å­¦ç¿’ã¯ã€Œå…¨ãƒ¢ãƒ‡ãƒ«éNaNã€ã‹ã¤ y ãŒ NaN ã§ãªã„è¡Œã®ã¿
    mask_fit = Z.notna().all(axis=1).values & ~np.isnan(y)
    if mask_fit.sum() < max(20, len(M)):  # è¡ŒãŒå°‘ãªã™ããŸã‚‰å¹³å‡ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return mean_blend(df_models), np.ones(len(M))/len(M)

    X_fit = Z.loc[mask_fit, M].to_numpy()
    y_fit = y[mask_fit]
    reg = LinearRegression(positive=True, fit_intercept=False)
    reg.fit(X_fit, y_fit)
    w = reg.coef_.copy()
    w = np.maximum(w, 0)
    s = w.sum()
    w = w / s if s > 0 else np.ones_like(w)/len(w)

    # æ¨è«–ï¼šè¡Œã”ã¨ã«åˆ©ç”¨å¯èƒ½ãªåˆ—ã®é‡ã¿ã ã‘ã§å†æ­£è¦åŒ–
    X_all = Z.to_numpy()
    pred = np.full(len(df_models), np.nan)
    for i in range(len(df_models)):
        row = X_all[i, :]
        avail = ~np.isnan(row)
        if not avail.any():
            continue
        ww = w[avail]
        xx = row[avail]
        s = ww.sum()
        pred[i] = (ww @ xx) / s if s > 0 else np.nanmean(xx)
    return pred, list(w)
def nnls_positive_blend(X_sel: pd.DataFrame, y: np.ndarray) -> tuple[np.ndarray, np.ndarray]:
    """
    é¸æŠœæ¸ˆã¿åˆ— X_sel ã«ã¤ã„ã¦ã€éè² ãƒ»ç·å’Œ1ã®é‡ã¿ã§ OOF ã‚’ç·šå½¢çµåˆ
    å­¦ç¿’: ã™ã¹ã¦ã®é¸æŠœåˆ—ãŒéNaN & y éNaN ã®è¡Œã®ã¿
    æ¨è«–: è¡Œã”ã¨ã«åˆ©ç”¨å¯èƒ½åˆ—ã§é‡ã¿ã‚’å†æ­£è¦åŒ–
    """
    if X_sel.shape[1] == 0:
        return np.full(len(y), np.nan), np.array([])

    mask_fit = (~np.isnan(y)) & X_sel.notna().all(axis=1).values
    if mask_fit.sum() < max(20, X_sel.shape[1]):  # ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã™ãã‚‹å ´åˆã¯å¹³å‡ã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        pred = X_sel.mean(axis=1, skipna=True).to_numpy()
        w = np.ones(X_sel.shape[1]) / X_sel.shape[1]
        return pred, w

    X_fit = X_sel.loc[mask_fit].to_numpy()
    y_fit = y[mask_fit].astype(float)

    reg = Ridge(alpha=1e-6, fit_intercept=False, positive=True)
    reg.fit(X_fit, y_fit)
    w = np.maximum(reg.coef_.copy(), 0.0)
    s = w.sum()
    w = (w / s) if s > 0 else np.ones_like(w) / len(w)

    X_all = X_sel.to_numpy()
    pred = np.full(len(y), np.nan)
    for i in range(len(y)):
        row = X_all[i, :]
        avail = ~np.isnan(row)
        if not avail.any():
            continue
        ww = w[avail]
        xx = row[avail]
        s = ww.sum()
        pred[i] = (ww @ xx) / s if s > 0 else np.nanmean(xx)
    return pred, w
# ---------------------------------------------------------------------------

def blend_per_target_safe(train, dfs_by_model, targets, method="nnls",
                          rel_delta=0.02, abs_eps=None, max_keep=None, verbose=True):
    """
    å¤‰æ›´ç‚¹:
      - å„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã§ 'æœ€è‰¯MAEã«è¿‘ã„' ãƒ¢ãƒ‡ãƒ«ã ã‘é¸æŠœã—ã¦ã‹ã‚‰ãƒ–ãƒ¬ãƒ³ãƒ‰
      - é¸ã°ã‚Œãªã‹ã£ãŸãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã¯ 0
      - id ã§çªåˆï¼ˆOOFã¯ id ãŒãªã‘ã‚Œã° SMILESâ†’id ã‚’è£œå®Œï¼‰
    """
    outputs = {}
    tgt_w = {}

    for tgt in targets:
        df = prepare_oof_table(train, dfs_by_model, tgt)  # ['id','y_true', m1, m2,...]
        y = df['y_true'].to_numpy().astype(float)
        X = df.drop(columns=['id','y_true']).astype(float)

        # è©•ä¾¡å¯èƒ½è¡Œ
        avail_row = X.notna().any(axis=1).values & ~np.isnan(y)
        if avail_row.sum() == 0:
            if verbose:
                print(f"[{tgt}] No evaluable rows (y or all models are NaN).")
            outputs[tgt] = pd.DataFrame({"id": df["id"], tgt: np.full(len(df), np.nan)})
            tgt_w[tgt] = {m: 0.0 for m in X.columns}
            continue

        # --- ã“ã“ãŒæ–°è¦ï¼šãƒ¢ãƒ‡ãƒ«é¸æŠœ ---
        mae_s = per_model_mae_table(X, y)
        keep = select_models_by_mae(mae_s, rel_delta=rel_delta, abs_eps=abs_eps,
                                    min_keep=1, max_keep=max_keep)
        if verbose:
            print(f"[{tgt}] best MAE by model:\n{mae_s.head(len(mae_s)).round(6)}")
            print(f"[{tgt}] selected: {keep}")

        X_sel = X[keep]

        # --- ãƒ–ãƒ¬ãƒ³ãƒ‰ ---
        if method in ("nnls", "ridge"):
            pred_full, w_sel = nnls_positive_blend(X_sel, y)
            # å…¨ãƒ¢ãƒ‡ãƒ«ã«å¯¾å¿œã™ã‚‹é‡ã¿è¾æ›¸ï¼ˆæœªæ¡ç”¨ã¯ 0ï¼‰
            w_all = {m: 0.0 for m in X.columns}
            for i, m in enumerate(keep):
                w_all[m] = float(w_sel[i])
            if verbose:
                ws = ", ".join([f"{m}:{w_all[m]:.3f}" for m in keep])
                print(f"[{tgt}] weights -> {ws}")
        elif method == "mean":
            pred_full = X_sel.mean(axis=1, skipna=True).to_numpy()
            w_all = {m: (1.0/len(keep) if m in keep else 0.0) for m in X.columns}
        elif method == "median":
            pred_full = X_sel.median(axis=1, skipna=True).to_numpy()
            w_all = {m: 0.0 for m in X.columns}  # å‚è€ƒå€¤ï¼šä¸­å¤®å€¤ãªã®ã§é‡ã¿ã‚’å‡ºã•ãªã„
        elif method == "rank_avg":
            R = X_sel.rank(method='average', na_option='keep', pct=True)
            pred_full = R.mean(axis=1, skipna=True).to_numpy()
            w_all = {m: (1.0/len(keep) if m in keep else 0.0) for m in X.columns}
        else:
            raise ValueError("unknown method")

        # OOFè©•ä¾¡ï¼ˆå‚è€ƒï¼‰
        mask_eval = avail_row & ~np.isnan(pred_full)
        mae = np.nan if mask_eval.sum() == 0 else mean_absolute_error(y[mask_eval], pred_full[mask_eval])
        if verbose:
            print(f"[{tgt}] OOF MAE: {mae:.6f} (n={mask_eval.sum()})")

        outputs[tgt] = pd.DataFrame({"id": df["id"], tgt: pred_full})
        tgt_w[tgt] = w_all

    return outputs, tgt_w

def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-exp", type=str, nargs="*", help="ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã™ã‚‹exp")
    parser.add_argument("-dir", type=str, default="20250830", help="outputs/ensemble é…ä¸‹ã«ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå")
    parser.add_argument("-upload_kaggle", action="store_true")
    parser.add_argument("-rel_delta", type=float, default=0.02)

    return parser.parse_args()

def main():
    targets = ["Tg", "Tc", "Rg", "FFV", "Density"]

    args = get_args()
    if len(args.exp) == 0:
        print("Not exists exp.")
        return
    
    
    train, _ = load_data(abspath / "data/raw")
    output_dir = abspath / "outputs"

    save_dir = output_dir / "ensemble" / args.dir
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    # ã“ã“å·®ã—æ›¿ãˆï¼šSMILES ã§ã¯ãªã id ã§æƒãˆã‚‹ï¼ˆSMILESã—ã‹ãªã„å ´åˆã ã‘è£œå®Œï¼‰
    dfs = {}
    for exp in args.exp:
        oof_df = pd.read_csv(output_dir / exp / "oof.csv")
        if "id" not in oof_df.columns:
            # å¤ã„å½¢å¼: SMILESâ†’id ã‚’è£œå®Œ
            tmp = train[["id","SMILES"]].merge(oof_df, how="left", on="SMILES")
            oof_df = tmp.drop(columns=["SMILES"])
        # id ãƒ™ãƒ¼ã‚¹ã§ä¿æŒ
        dfs[exp] = oof_df[["id"] + targets].copy()

    for target in targets:
        train[f"org_{target}"] = train[target]

    train["id"] = np.arange(len(train))

    # ç›¸å¯¾é–¾å€¤ã¯ä¾‹: 0.02 (=æœ€è‰¯MAEã®+2%ä»¥å†…ã®ã¿æ¡ç”¨)
    outs, w = blend_per_target_safe(
        train=train,
        dfs_by_model=dfs,
        targets=targets,
        method="nnls",         # mean/median/rank_avg ã‚‚å¯
        rel_delta=args.rel_delta,        # <-- ã“ã“ã‚’ 0.01~0.05 ã§èª¿æ•´
        abs_eps=None,          # ä½µç”¨ã—ãŸã„ãªã‚‰ e.g. abs_eps=0.05
        max_keep=None,         # ä¸Šé™ã‚’è¨­ã‘ãŸã„ã¨ãã¯æ•´æ•°æŒ‡å®šï¼ˆä¾‹: 3ï¼‰
        verbose=True
    )
    weight_w = {}

    for target in targets:
        weight_w[target] = {}
        for idx, exp in enumerate(args.exp):
            weight_w[target][exp] = w[target][exp]

    print(weight_w)

        
    with open(save_dir / "weights.json", "w") as f:
        json.dump(weight_w, f)
    
    oof_df = pd.DataFrame({
        "id": range(len(train))
    })

    for target in targets:
        df = outs[target]
        oof_df = oof_df.merge(df, how="left", on="id")

    results = vars(args)

    # CV è¨ˆç®—
    solution = train[["id"] + targets].copy()

    # è©•ä¾¡
    final_score = score(
        solution=solution,
        submission=oof_df,
    )
    print(f"\nğŸ“Š Final OOF Score (wMAE): {final_score:.6f}")
    results["score"] = round(final_score, 5)
    results["mae"] = {}

    for target in targets:
        solution.loc[solution[target] == NULL_FOR_SUBMISSION, target] = np.nan
        oof_df.loc[oof_df[target] == NULL_FOR_SUBMISSION, target] = np.nan
        y = solution[[target]].dropna()[target]
        y_pred = oof_df[oof_df["id"].isin(solution[["id", target]].dropna().index)][target]
        mae = mean_absolute_error(y, y_pred)
        print(f"{target} mae: {mae:.6f}")
        results["mae"][target] = round(mae, 5)

    print(results)
    with open(save_dir / "results.json", "w") as f:
        json.dump(results, f)


    if args.upload_kaggle:
        dataset_title = f"ensemble-weight-{args.dir}"
        dataset_id = f"koya346/{dataset_title}"
        create_kaggle_dataset_metadata(dataset_title, dataset_id, save_dir)
        upload_kaggle_dataset(dataset_id, save_dir)
    

    return 

if __name__ == "__main__":
    main()
