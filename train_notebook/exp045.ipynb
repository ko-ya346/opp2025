{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "634a0b10-991b-4d14-b39c-6507c6bcb418",
   "metadata": {},
   "source": [
    "- -> exp045\n",
    "- Ë©ï‰æ°„Éá„Éº„Çø„ÅØ `is_external=False` „ÅÆ„Åø\n",
    "- Ë©ï‰æ°„Éá„Éº„Çø„Å´Âê´„Åæ„Çå„Çã smiles „ÅØÈô§Â§ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa7a2c22-eb3e-479e-8f1f-9f9b50a176ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autotime\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7174264-b9fd-41b5-91fe-f5174dd30852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "is_kaggle_notebook = os.path.exists(\"/kaggle/input\")\n",
    "\n",
    "# ÂøÖË¶Å„Éë„ÉÉ„Ç±„Éº„Ç∏„Çí„Ç§„É≥„Çπ„Éà„Éº„É´\n",
    "if is_kaggle_notebook:\n",
    "    !pip install /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl\n",
    "    !pip install /kaggle/input/torch-geometric-2-6-1/torch_geometric-2.6.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3f929c2-afe2-4fcb-986b-6f50075faa7a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mko_ya346\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import wandb\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from rdkit import rdBase\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy import sparse\n",
    "\n",
    "rdBase.DisableLog(\"rdApp.warning\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de6f94aa-f4e5-4b14-8bad-f9f73cd60375",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "pr_number = 1\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ccadd16-5880-496e-a632-4d5940b2266c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "if is_kaggle_notebook:\n",
    "    module_path = f\"/kaggle/input/myproject-pr-{pr_number:04}\"\n",
    "    !mkdir src\n",
    "    !cp -r $module_path/* src/\n",
    "    src_path = \"./\"\n",
    "else:\n",
    "    src_path = \"../\"\n",
    "\n",
    "sys.path.append(src_path)\n",
    "\n",
    "from src.data import load_data, add_descriptors, add_external_data, make_smile_canonical, add_maccs, add_augumented_gmm, add_graph_features, add_count_atoms\n",
    "from src.model import train_lgb_for_target, save_lgb_model\n",
    "from src.utils import NULL_FOR_SUBMISSION, generate_scaffold, score, add_scaffold_kfold, scaffold_cv_split, get_useless_cols\n",
    "from src.utils.upload_kaggle_dataset import (\n",
    "    create_kaggle_dataset_metadata,\n",
    "    upload_kaggle_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "915c44aa-01b8-4b84-a377-6cf8d5e9a31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir ../outputs/exp045\n",
    "# !cp ../outputs/exp044/train.csv ../outputs/exp044/train.csv\n",
    "\n",
    "# !rm ../outputs/exp045/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ef59d0f-f03f-4636-9839-c8cbc2b4cfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = \"exp045\"\n",
    "notes = \"\"\n",
    "model_name = \"lgb\"\n",
    "\n",
    "config = {\n",
    "    \"debug\": debug,\n",
    "    \"n_splits\": 3,\n",
    "    \"num_epochs\": 1000,\n",
    "    \"batch_size\": 128,\n",
    "    \"drop_ratio\": 0.5,\n",
    "    \"force_update_train\": False,\n",
    "    \"augumented_gmm\": False,\n",
    "    \"is_complement\": True,\n",
    "    \"remove_external_cv\": True,\n",
    "}\n",
    "\n",
    "dataset_title = f\"model-{exp}\"\n",
    "dataset_id = f\"koya346/{dataset_title}\"\n",
    "\n",
    "if is_kaggle_notebook:\n",
    "    config[\"debug\"] = False\n",
    "\n",
    "if config[\"debug\"]:\n",
    "    config[\"n_splits\"] = 2\n",
    "    config[\"num_epochs\"] = 10\n",
    "\n",
    "targets = [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"]\n",
    "org_target_cols = [f\"org_{target}\" for target in targets]        \n",
    "\n",
    "# TODO: Â≠¶Áøí„Éë„É©„É°„Éº„ÇøÂÆöÁæ©\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"mae\",\n",
    "    \"verbosity\": -1,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"max_depth\": 7,\n",
    "    \"seed\": 42,\n",
    "    \"subsample\": 0.7,\n",
    "    \"colsample_bytree\": 0.6,\n",
    "    \"num_boost_round\": 20000,\n",
    "}\n",
    "\n",
    "config.update(params)\n",
    "pre_params = copy.deepcopy(params)\n",
    "pre_params[\"num_boost_round\"] = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16a10533-77cf-436d-8abf-a3ae32a17545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kouya-takahashi/kaggle/opp2025/notebook/wandb/run-20250826_230031-yidnlslu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ko_ya346/opp2025/runs/yidnlslu' target=\"_blank\">exp045_lgb</a></strong> to <a href='https://wandb.ai/ko_ya346/opp2025' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ko_ya346/opp2025' target=\"_blank\">https://wandb.ai/ko_ya346/opp2025</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ko_ya346/opp2025/runs/yidnlslu' target=\"_blank\">https://wandb.ai/ko_ya346/opp2025/runs/yidnlslu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bit cols:  0\n",
      "remove cols:  0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['fold'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_860658/1662171202.py\u001b[0m in \u001b[0;36m<cell line: 175>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morg_target_cols\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"org_SMILES\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SMILES\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fold\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"is_external\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0moof_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle/opp2025/myenv/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5586\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5587\u001b[0m         \"\"\"\n\u001b[0;32m-> 5588\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   5589\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5590\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle/opp2025/myenv/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4805\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4806\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4807\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4809\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle/opp2025/myenv/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4847\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4848\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4849\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4850\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle/opp2025/myenv/lib/python3.10/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7135\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7136\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask].tolist()} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7137\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['fold'] not found in axis\""
     ]
    }
   ],
   "source": [
    "wandb_name = f\"{exp}_{model_name}\" if not config[\"debug\"] else f\"{exp}_{model_name}_debug\"\n",
    "wandb.init(project=\"opp2025\", name=wandb_name, config=config)\n",
    "wandb.log({\"Notes\": notes})\n",
    "\n",
    "# ---------------------------\n",
    "# „É°„Ç§„É≥Âá¶ÁêÜ\n",
    "# ---------------------------\n",
    "if config[\"debug\"]:\n",
    "    output_path = Path(\"/home/kouya-takahashi/kaggle/opp2025/outputs\") / exp / \"debug\"\n",
    "else:\n",
    "    output_path = Path(\"/home/kouya-takahashi/kaggle/opp2025/outputs\") / exp\n",
    "\n",
    "model_output_path = output_path / \"model_cv\"\n",
    "if not os.path.exists(model_output_path):\n",
    "    os.makedirs(model_output_path)\n",
    "\n",
    "if is_kaggle_notebook:\n",
    "    # kaggle notebook\n",
    "    data_dir = Path(\"/kaggle/input\")\n",
    "else:\n",
    "    # local\n",
    "    data_dir = Path(\"/home/kouya-takahashi/kaggle/opp2025/data/raw\")\n",
    "\n",
    "# Â≠¶Áøí„Éá„Éº„ÇøÁî®ÊÑè\n",
    "\n",
    "if os.path.exists(output_path / \"train.csv\") and not config[\"force_update_train\"]:\n",
    "    train = pd.read_csv(output_path / \"train.csv\")\n",
    "else:\n",
    "    train, _ = load_data(data_dir)\n",
    "    # Â§ñÈÉ®„Éá„Éº„ÇøÂà§ÂÆöÁî®„Å´ÂÖÉ„ÅÆÁõÆÁöÑÂ§âÊï∞„Çí‰øùÊåÅ„Åó„Å¶„Åä„Åè\n",
    "    # Â§ñÈÉ®„Éá„Éº„ÇøÂà§ÂÆö„ÅØ scaffold_cv_split ÂÜÖ„ÅßË°å„ÅÜ\n",
    "    for target in targets:\n",
    "        train[f\"org_{target}\"] = train[target]\n",
    "    \n",
    "\n",
    "    train[\"org_SMILES\"] = train[\"SMILES\"]\n",
    "    train[\"SMILES\"] = train[\"SMILES\"].apply(make_smile_canonical)\n",
    "    if config[\"debug\"]:\n",
    "        # ÂêÑ„Çø„Éº„Ç≤„ÉÉ„Éà„ÅåÊ¨†Êêç„Åó„Å¶„ÅÑ„Å™„ÅÑ„Éá„Éº„Çø„Çí30 ‰ª∂„Åö„Å§Âèñ„ÇäÂá∫„Åô\n",
    "        tmp_dfs = []\n",
    "        for target in targets:\n",
    "            cond = train[target].notnull()\n",
    "            tmp_dfs.append(train[cond].iloc[:30])\n",
    "        train = pd.concat(tmp_dfs).reset_index(drop=True)\n",
    "    else:\n",
    "        print(train.shape)\n",
    "        external_data_dict = [\n",
    "            {\n",
    "                \"ex_path\": data_dir / \"neurips-open-polymer-prediction-2025/train_supplement/dataset3.csv\",\n",
    "                \"col\": \"Tg\",\n",
    "            },\n",
    "            {\n",
    "                \"ex_path\": data_dir / \"neurips-open-polymer-prediction-2025/train_supplement/dataset1.csv\",\n",
    "                \"col\": \"Tc\",\n",
    "                \"rename_d\": {\"TC_mean\": \"Tc\"},\n",
    "            },\n",
    "            {\n",
    "                \"ex_path\": data_dir / \"neurips-open-polymer-prediction-2025/train_supplement/dataset4.csv\",\n",
    "                \"col\": \"FFV\",\n",
    "            },\n",
    "            {\n",
    "                \"ex_path\": data_dir / \"tg-smiles-pid-polymer-class/TgSS_enriched_cleaned.csv\",\n",
    "                \"col\": \"Tg\",\n",
    "            },\n",
    "            {\n",
    "                \"ex_path\": data_dir / \"smiles-extra-data/data_dnst1.xlsx\",\n",
    "                \"col\": \"Density\",\n",
    "                \"rename_d\": {\"density(g/cm3)\": \"Density\"}, \n",
    "            },\n",
    "            {\n",
    "                \"ex_path\": data_dir / \"smiles-extra-data/data_tg3.xlsx\",\n",
    "                \"col\": \"Tg\",\n",
    "                \"rename_d\": {\"Tg [K]\": \"Tg\"}, \n",
    "            },\n",
    "            {\n",
    "                \"ex_path\": data_dir / \"smiles-extra-data/JCIM_sup_bigsmiles.csv\",\n",
    "                \"col\": \"Tg\",\n",
    "                \"rename_d\": {\"Tg (C)\": \"Tg\"}, \n",
    "            },\n",
    "        ]\n",
    "        for d in external_data_dict:\n",
    "            print(f\"ex_path: {str(d['ex_path'])}\")\n",
    "            train = add_external_data(\n",
    "                df=train,\n",
    "                ex_path=d.get(\"ex_path\"),\n",
    "                col=d.get(\"col\"),\n",
    "                rename_d=d.get(\"rename_d\"),\n",
    "                is_complement=config[\"is_complement\"]\n",
    "            )\n",
    "            print(\"after train.shape: \", train.shape)\n",
    "\n",
    "    train[\"is_external\"] = train[\"id\"].isnull()\n",
    "    train = add_maccs(train)\n",
    "\n",
    "    # rdkit „ÅÆË®òËø∞Â≠ê, morgan finger print\n",
    "    train = add_descriptors(train, radius=2, fp_size=1024)\n",
    "\n",
    "    new_cols = []\n",
    "    seen = {}\n",
    "    for col in train.columns:\n",
    "        if col in seen:\n",
    "            seen[col] += 1\n",
    "            new_cols.append(f\"{col}_{seen[col]}\")\n",
    "        else:\n",
    "            seen[col] = 0\n",
    "            new_cols.append(col)\n",
    "    \n",
    "    train.columns = new_cols\n",
    "    \n",
    "    # „Ç∞„É©„ÉïÁâπÂæ¥Èáè\n",
    "    train = add_graph_features(train)\n",
    "    train = add_count_atoms(train)\n",
    "    \n",
    "    train[\"id\"] = np.arange(len(train))\n",
    "    features = train.drop(targets + org_target_cols + [\"id\", \"org_SMILES\", \"SMILES\", \"is_external\"], axis=1).columns\n",
    "    for col in features:\n",
    "        if train[col].dtype == \"object\":\n",
    "            train[col] = pd.to_numeric(train[col], errors=\"coerce\")\n",
    "    useless_cols = get_useless_cols(train.drop(targets + org_target_cols + [\"id\", \"org_SMILES\", \"SMILES\", \"is_external\"], axis=1))\n",
    "    \n",
    "    train = train.drop(useless_cols, axis=1)\n",
    "    \n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    train.to_csv(output_path / \"train.csv\", index=False)\n",
    "    print(\"Saved train.csv\")\n",
    "\n",
    "# ‰∏çË¶Å„Å™„Éì„ÉÉ„ÉàÂàó„ÇíÈô§Âéª\n",
    "bit_cols = []\n",
    "remove_cols = []\n",
    "\n",
    "for col in train.drop(targets + org_target_cols + [\"id\", \"org_SMILES\", \"SMILES\", \"is_external\"], axis=1).columns:\n",
    "    if len(train[col].unique()) != 2:\n",
    "        continue\n",
    "    if np.all(train[col].unique() == np.array([0, 1])):\n",
    "        bit_cols.append(col)\n",
    "        p = train[col].mean()\n",
    "        if p > 0.01 and p < 0.99:\n",
    "            continue\n",
    "        remove_cols.append(col)\n",
    "print(\"bit cols: \", len(bit_cols))\n",
    "print(\"remove cols: \", len(remove_cols))\n",
    "\n",
    "train = train.drop(remove_cols, axis=1)\n",
    "\n",
    "# # „Éì„ÉÉ„ÉàÂàó„ÅÆ‰∫ãÂâçÂàà„ÇäËæº„Åø\n",
    "# bit_col_groups = [\n",
    "#     {\"name\": \"mfp\", \"n_components\": 128}, \n",
    "#     {\"name\": \"maccs\", \"n_components\": 32}\n",
    "# ]\n",
    "\n",
    "# for d in bit_col_groups:\n",
    "#     print(d)\n",
    "#     cols = [col for col in train.columns if d[\"name\"] in col]\n",
    "#     df = train[cols]\n",
    "#     print(len(cols))\n",
    "    \n",
    "#     X_bids = sparse.csr_matrix(df.values)\n",
    "#     svd = TruncatedSVD(n_components=d[\"n_components\"], random_state=42)\n",
    "#     X_svd = svd.fit_transform(X_bids)\n",
    "#     X_svd_df = pd.DataFrame(X_svd, columns=[f'{d[\"name\"]}_{idx}' for idx in range(d[\"n_components\"])])\n",
    "#     print(X_svd_df.shape)\n",
    "\n",
    "#     # ÂÖÉ„ÅÆÁ≥ªÂàó„ÇíËêΩ„Å®„Åô\n",
    "#     train = train.drop(cols, axis=1)\n",
    "#     # ÂúßÁ∏Æ„Åó„ÅüÁ≥ªÂàó„ÇíËøΩÂä†\n",
    "#     train = pd.concat([train, X_svd_df], axis=1)\n",
    "#     print(train.shape)\n",
    "\n",
    "# Ë®àÁÆóÊ∏à„ÅÆ fold „ÇíÁ™ÅÂêà\n",
    "\n",
    "folds = pd.read_csv(\"../data/preprocess/fold/folds.csv\")\n",
    "train = train.merge(folds[[\"SMILES\", \"fold\"]], how=\"left\", on=\"SMILES\")\n",
    "\n",
    "\n",
    "features = train.drop(targets + org_target_cols + [\"id\", \"org_SMILES\", \"SMILES\", \"fold\", \"is_external\"], axis=1).columns\n",
    "print(len(features))\n",
    "oof_dfs = []\n",
    "\n",
    "loss_table_wandb = wandb.Table([\"exp\", \"model_name\", \"fold\", \"target\", \"mae\", \"mse\"])\n",
    "all_loss_tables = []\n",
    "mae_dict = {}\n",
    "all_models = {}\n",
    "\n",
    "for idx, target_col in enumerate(targets):\n",
    "    loss_tables = []\n",
    "    print(f\"\\n=== Training for target: {target_col} ===\")\n",
    "\n",
    "    df_train = train[train[target_col].notnull()].reset_index(drop=True)\n",
    "    X = df_train[features]\n",
    "    y = df_train[target_col]\n",
    "    oof = np.full(len(X), np.nan, dtype=float)\n",
    "    \n",
    "    models = []\n",
    "\n",
    "    for fold, tr_idx, val_idx in scaffold_cv_split(df_train, target=target_col, n_splits=config[\"n_splits\"], remove_external=config[\"remove_external_cv\"]):\n",
    "        if len(tr_idx) == 0 or len(val_idx) == 0:\n",
    "            print(f\"Skip fold... tr_idx: {len(tr_idx)}, val_idx: {len(val_idx)}\")\n",
    "            continue\n",
    "        loss_table = {}\n",
    "        print(f\"fold: {fold + 1}\")\n",
    "        if target_col in targets and not config[\"debug\"]:\n",
    "            X_train_pre = X.iloc[tr_idx]\n",
    "            y_train_pre = y.iloc[tr_idx]\n",
    "    \n",
    "            dtrain_pre = lgb.Dataset(X_train_pre, label=y_train_pre)\n",
    "    \n",
    "            # valid „Éá„Éº„Çø„Çí‰Ωø„Çè„Åö„Å´Â≠¶Áøí\n",
    "            pre_model = lgb.train(\n",
    "                pre_params,\n",
    "                dtrain_pre,\n",
    "            )\n",
    "    \n",
    "            # ÂØÑ‰∏éÂ∫¶„Åå 0 „Çà„ÇäÂ§ß„Åç„ÅÑÁâπÂæ¥Èáè„ÇíÂèñ„ÇäÂá∫„Åô\n",
    "            feature_importance = pre_model.feature_importance()\n",
    "            print(np.sum(feature_importance == 0) / len(feature_importance))\n",
    "            \n",
    "            feature_name = pre_model.feature_name()\n",
    "            use_features = [feature_name[idx] for idx in range(len(feature_name)) if feature_importance[idx] > 0]\n",
    "        else:\n",
    "            use_features = features\n",
    "        print(len(use_features))        \n",
    "\n",
    "        # ÁâπÂæ¥ÈáèÈÅ∏Êäû„Åó„Å¶ valid „Éá„Éº„Çø„Å®„Å®„ÇÇ„Å´Â≠¶Áøí\n",
    "        X_train, X_val = X.iloc[tr_idx][use_features], X.iloc[val_idx][use_features]\n",
    "        y_train, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n",
    "\n",
    "        if config[\"augumented_gmm\"]:\n",
    "            X_train, y_train = add_augumented_gmm(X_train, y_train)    \n",
    "        \n",
    "        dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "        dval = lgb.Dataset(X_val, label=y_val, reference=dtrain)\n",
    "\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            valid_sets=[dtrain, dval],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=50),\n",
    "                lgb.log_evaluation(200)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        save_lgb_model(model, str(model_output_path / f\"model_{target_col}_{fold}.txt\"))\n",
    "\n",
    "        pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "        oof[val_idx] = pred\n",
    "\n",
    "        mse = mean_squared_error(y_val, pred)\n",
    "        mae = mean_absolute_error(y_val, pred)\n",
    "        print(f\"fold: {fold}, target: {target_col}, mae: {mae}\")\n",
    "        loss_table[\"fold\"] = fold\n",
    "        loss_table[\"target\"] = target_col\n",
    "        loss_table[\"mae\"] = mae\n",
    "        loss_table[\"mse\"] = mse\n",
    "\n",
    "        loss_tables.append(loss_table)\n",
    "        models.append(model)\n",
    "\n",
    "    # Ë©ï‰æ°„Å´‰Ωø„ÅÜ„ÅÆ„ÅØÂÖÉ„Éá„Éº„Çø„ÅÆ„Åø\n",
    "    cond = (~np.isnan(oof)) & (df_train[f\"org_{target_col}\"].notnull())\n",
    "    y_true = y[cond]\n",
    "    y_pred = oof[cond]\n",
    "    score_mse = mean_squared_error(y_true, y_pred)\n",
    "    score_mae = mean_absolute_error(y_true, y_pred)\n",
    "    print(f\"RMSE for {target_col}: {score_mse:.4f}\")\n",
    "    print(f\"MAE for {target_col}: {score_mae:.4f}\")\n",
    "    mae_dict[target_col] = score_mae\n",
    "\n",
    "    for loss_table in loss_tables:\n",
    "        loss_table_wandb.add_data(exp, model_name, loss_table[\"fold\"], loss_table[\"target\"], loss_table[\"mae\"], loss_table[\"mse\"])\n",
    "    all_loss_tables += loss_tables\n",
    "\n",
    "    oof_df = pd.DataFrame({\n",
    "        \"id\": df_train[\"id\"].values,\n",
    "        target_col: oof\n",
    "    })\n",
    "    oof_dfs.append(oof_df)   \n",
    "\n",
    "    all_models[target_col] = models\n",
    "\n",
    "wandb.log({\"fold_target_losses\": loss_table_wandb})\n",
    "# target ÊØé„ÅÆ Âπ≥Âùá mae „ÇíË®òÈå≤\n",
    "for target in targets:\n",
    "    key_name = f\"{target}_mean_mae\"\n",
    "    mae_values = mae_dict[target]\n",
    "    # mae_values = [d[\"mae\"] for d in all_loss_tables if d[\"target\"] == target]\n",
    "    wandb.log({key_name: np.mean(mae_values)})\n",
    "\n",
    "# CV Ë®àÁÆó\n",
    "cond = ~train[\"is_external\"]\n",
    "oof_df = pd.DataFrame()\n",
    "\n",
    "# ÂÖÉ„ÅÆÁõÆÁöÑÂ§âÊï∞„ÇÇÂÖ•„Çå„Å¶„Åä„Åè\n",
    "for target in targets:\n",
    "    oof_df[f\"org_{target}\"] = train.loc[cond, f\"org_{target}\"]\n",
    "\n",
    "oof_df[\"id\"] = train.loc[cond, \"id\"]\n",
    "\n",
    "# Ê≠£Ë¶èÂåñÂâç„ÅÆ SMILES\n",
    "oof_df[\"SMILES\"] = train.loc[cond, \"org_SMILES\"]\n",
    "\n",
    "for i_oof in oof_dfs:\n",
    "    oof_df = oof_df.merge(i_oof, on=\"id\", how=\"left\")\n",
    "\n",
    "# ‰∫àÊ∏¨„Åó„Å™„Åã„Å£„ÅüÈÉ®ÂàÜ„ÅØ null „Å´„Åó„Å¶„Åä„Åè\n",
    "for target in targets:\n",
    "    oof_df.loc[oof_df[target] == 0, target] = np.nan\n",
    "\n",
    "oof_df.to_csv(output_path / \"oof.csv\", index=False)\n",
    "\n",
    "solution = train.loc[cond, [\"id\"] + org_target_cols].copy()\n",
    "solution.columns = [\"id\"] + targets\n",
    "\n",
    "# solution = solution.fillna(NULL_FOR_SUBMISSION)\n",
    "\n",
    "# oof_df = oof_df.fillna(NULL_FOR_SUBMISSION)\n",
    "\n",
    "# Ë©ï‰æ°\n",
    "final_score = score(\n",
    "    solution=solution,\n",
    "    submission=oof_df,\n",
    ")\n",
    "print(f\"\\nüìä Final OOF Score (wMAE): {final_score:.6f}\")\n",
    "wandb.log({\"wMAE\": final_score})\n",
    "\n",
    "# target ÊØé„ÅÆ best_iteration „Çí‰øùÂ≠ò„Åô„Çã„ÄÇ‰øùÂ≠ò„Åó„Åü„É¢„Éá„É´„Å´„ÅØË®òÈå≤„Åï„Çå„Å¶„Å™„Åã„Å£„Åü\n",
    "best_iterations = {}\n",
    "for target in targets:\n",
    "    target_best_iterations = [model.best_iteration for model in all_models[target]]\n",
    "    best_iterations[target] = target_best_iterations\n",
    "print(best_iterations)\n",
    "\n",
    "with open(output_path / \"best_iterations.json\", \"w\") as f:\n",
    "    json.dump(best_iterations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe64d9d-57f3-4c66-bcb9-120528b2ce27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opp2025_myenv",
   "language": "python",
   "name": "opp2025_myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
