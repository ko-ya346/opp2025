{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "634a0b10-991b-4d14-b39c-6507c6bcb418",
   "metadata": {},
   "source": [
    "- -> exp038\n",
    "- fold „ÅÆÂàá„ÇäÊñπ‰øÆÊ≠£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa7a2c22-eb3e-479e-8f1f-9f9b50a176ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# %load_ext autotime\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7174264-b9fd-41b5-91fe-f5174dd30852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "is_kaggle_notebook = os.path.exists(\"/kaggle/input\")\n",
    "\n",
    "# ÂøÖË¶Å„Éë„ÉÉ„Ç±„Éº„Ç∏„Çí„Ç§„É≥„Çπ„Éà„Éº„É´\n",
    "if is_kaggle_notebook:\n",
    "    !pip install /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl\n",
    "    !pip install /kaggle/input/torch-geometric-2-6-1/torch_geometric-2.6.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3f929c2-afe2-4fcb-986b-6f50075faa7a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import wandb\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from rdkit import rdBase\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "rdBase.DisableLog(\"rdApp.warning\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de6f94aa-f4e5-4b14-8bad-f9f73cd60375",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "pr_number = 1\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ccadd16-5880-496e-a632-4d5940b2266c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "if is_kaggle_notebook:\n",
    "    module_path = f\"/kaggle/input/myproject-pr-{pr_number:04}\"\n",
    "    !mkdir src\n",
    "    !cp -r $module_path/* src/\n",
    "    src_path = \"./\"\n",
    "else:\n",
    "    src_path = \"../\"\n",
    "\n",
    "sys.path.append(src_path)\n",
    "\n",
    "from src.data import load_data, add_descriptors, add_external_data, make_smile_canonical, add_maccs, add_augumented_gmm, add_graph_features, add_count_atoms\n",
    "from src.model import train_lgb_for_target, save_lgb_model\n",
    "from src.utils import NULL_FOR_SUBMISSION, generate_scaffold, score, add_scaffold_kfold, scaffold_cv_split, get_useless_cols\n",
    "from src.utils.upload_kaggle_dataset import (\n",
    "    create_kaggle_dataset_metadata,\n",
    "    upload_kaggle_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ef59d0f-f03f-4636-9839-c8cbc2b4cfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = \"exp041-3\"\n",
    "notes = \"ÁâπÂæ¥ÈáèÁµû„Çã\"\n",
    "model_name = \"lgb\"\n",
    "\n",
    "config = {\n",
    "    \"debug\": debug,\n",
    "    \"n_splits\": 5,\n",
    "    \"num_epochs\": 1000,\n",
    "    \"batch_size\": 128,\n",
    "    \"drop_ratio\": 0.5,\n",
    "    \"force_update_train\": False,\n",
    "    \"augumented_gmm\": True,\n",
    "    \"is_complement\": True\n",
    "}\n",
    "\n",
    "dataset_title = f\"model-{exp}\"\n",
    "dataset_id = f\"koya346/{dataset_title}\"\n",
    "\n",
    "if is_kaggle_notebook:\n",
    "    config[\"debug\"] = False\n",
    "\n",
    "if config[\"debug\"]:\n",
    "    config[\"n_splits\"] = 2\n",
    "    config[\"num_epochs\"] = 10\n",
    "\n",
    "targets = [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"]\n",
    "\n",
    "# TODO: Â≠¶Áøí„Éë„É©„É°„Éº„ÇøÂÆöÁæ©\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"mae\",\n",
    "    \"verbosity\": -1,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"max_depth\": 7,\n",
    "    \"seed\": 42,\n",
    "    \"subsample\": 0.7,\n",
    "    \"colsample_bytree\": 0.6,\n",
    "    \"num_boost_round\": 20000,\n",
    "}\n",
    "\n",
    "config.update(params)\n",
    "pre_params = copy.deepcopy(params)\n",
    "pre_params[\"num_boost_round\"] = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a10533-77cf-436d-8abf-a3ae32a17545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Density_mean_mae</td><td>‚ñÅ</td></tr><tr><td>FFV_mean_mae</td><td>‚ñÅ</td></tr><tr><td>Rg_mean_mae</td><td>‚ñÅ</td></tr><tr><td>Tc_mean_mae</td><td>‚ñÅ</td></tr><tr><td>Tg_mean_mae</td><td>‚ñÅ</td></tr><tr><td>wMAE</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Density_mean_mae</td><td>0.05383</td></tr><tr><td>FFV_mean_mae</td><td>0.00717</td></tr><tr><td>Notes</td><td>ÁâπÂæ¥ÈáèÁµû„Çã</td></tr><tr><td>Rg_mean_mae</td><td>1.91609</td></tr><tr><td>Tc_mean_mae</td><td>0.03716</td></tr><tr><td>Tg_mean_mae</td><td>32.40163</td></tr><tr><td>wMAE</td><td>0.06173</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">exp041-3_lgb</strong> at: <a href='https://wandb.ai/ko_ya346/opp2025/runs/o4vnf33v' target=\"_blank\">https://wandb.ai/ko_ya346/opp2025/runs/o4vnf33v</a><br> View project at: <a href='https://wandb.ai/ko_ya346/opp2025' target=\"_blank\">https://wandb.ai/ko_ya346/opp2025</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250823_220224-o4vnf33v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kouya-takahashi/kaggle/opp2025/notebook/wandb/run-20250823_222838-fkpwve85</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ko_ya346/opp2025/runs/fkpwve85' target=\"_blank\">exp042_lgb</a></strong> to <a href='https://wandb.ai/ko_ya346/opp2025' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ko_ya346/opp2025' target=\"_blank\">https://wandb.ai/ko_ya346/opp2025</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ko_ya346/opp2025/runs/fkpwve85' target=\"_blank\">https://wandb.ai/ko_ya346/opp2025/runs/fkpwve85</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7973, 7)\n",
      "ex_path: /home/kouya-takahashi/kaggle/opp2025/data/raw/neurips-open-polymer-prediction-2025/train_supplement/dataset3.csv\n",
      "after train.shape:  (8019, 7)\n",
      "ex_path: /home/kouya-takahashi/kaggle/opp2025/data/raw/neurips-open-polymer-prediction-2025/train_supplement/dataset1.csv\n",
      "after train.shape:  (8148, 7)\n",
      "ex_path: /home/kouya-takahashi/kaggle/opp2025/data/raw/neurips-open-polymer-prediction-2025/train_supplement/dataset4.csv\n",
      "after train.shape:  (8972, 7)\n",
      "ex_path: /home/kouya-takahashi/kaggle/opp2025/data/raw/tg-smiles-pid-polymer-class/TgSS_enriched_cleaned.csv\n",
      "after train.shape:  (10343, 7)\n",
      "ex_path: /home/kouya-takahashi/kaggle/opp2025/data/raw/smiles-extra-data/data_tg3.xlsx\n",
      "after train.shape:  (10842, 7)\n",
      "ex_path: /home/kouya-takahashi/kaggle/opp2025/data/raw/smiles-extra-data/JCIM_sup_bigsmiles.csv\n",
      "after train.shape:  (10854, 7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45fb44332ba54803a5c6ab454a107b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating maccs:   0%|          | 0/10854 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6730c87d654bfbb12aea7be67204ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating descriptors:   0%|          | 0/10854 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_name = f\"{exp}_{model_name}\" if not config[\"debug\"] else f\"{exp}_{model_name}_debug\"\n",
    "wandb.init(project=\"opp2025\", name=wandb_name, config=config)\n",
    "wandb.log({\"Notes\": notes})\n",
    "\n",
    "# ---------------------------\n",
    "# „É°„Ç§„É≥Âá¶ÁêÜ\n",
    "# ---------------------------\n",
    "if config[\"debug\"]:\n",
    "    output_path = Path(\"/home/kouya-takahashi/kaggle/opp2025/outputs\") / exp / \"debug\"\n",
    "else:\n",
    "    output_path = Path(\"/home/kouya-takahashi/kaggle/opp2025/outputs\") / exp\n",
    "\n",
    "model_output_path = output_path / \"model_cv\"\n",
    "if not os.path.exists(model_output_path):\n",
    "    os.makedirs(model_output_path)\n",
    "\n",
    "if is_kaggle_notebook:\n",
    "    # kaggle notebook\n",
    "    data_dir = Path(\"/kaggle/input\")\n",
    "else:\n",
    "    # local\n",
    "    data_dir = Path(\"/home/kouya-takahashi/kaggle/opp2025/data/raw\")\n",
    "\n",
    "# Â≠¶Áøí„Éá„Éº„ÇøÁî®ÊÑè\n",
    "\n",
    "if os.path.exists(output_path / \"train.csv\") and not config[\"force_update_train\"]:\n",
    "    train = pd.read_csv(output_path / \"train.csv\")\n",
    "else:\n",
    "    train, _ = load_data(data_dir)\n",
    "    train[\"SMILES\"] = train[\"SMILES\"].apply(make_smile_canonical)\n",
    "\n",
    "    if config[\"debug\"]:\n",
    "        # ÂêÑ„Çø„Éº„Ç≤„ÉÉ„Éà„ÅåÊ¨†Êêç„Åó„Å¶„ÅÑ„Å™„ÅÑ„Éá„Éº„Çø„Çí30 ‰ª∂„Åö„Å§Âèñ„ÇäÂá∫„Åô\n",
    "        tmp_dfs = []\n",
    "        for target in targets:\n",
    "            cond = train[target].notnull()\n",
    "            tmp_dfs.append(train[cond].iloc[:30])\n",
    "        train = pd.concat(tmp_dfs).reset_index(drop=True)\n",
    "    else:\n",
    "        print(train.shape)\n",
    "        external_data_dict = [\n",
    "            {\n",
    "                \"ex_path\": data_dir / \"neurips-open-polymer-prediction-2025/train_supplement/dataset3.csv\",\n",
    "                \"col\": \"Tg\",\n",
    "            },\n",
    "            {\n",
    "                \"ex_path\": data_dir / \"neurips-open-polymer-prediction-2025/train_supplement/dataset1.csv\",\n",
    "                \"col\": \"Tc\",\n",
    "                \"rename_d\": {\"TC_mean\": \"Tc\"},\n",
    "            },\n",
    "            {\n",
    "                \"ex_path\": data_dir / \"neurips-open-polymer-prediction-2025/train_supplement/dataset4.csv\",\n",
    "                \"col\": \"FFV\",\n",
    "            },\n",
    "            {\n",
    "                \"ex_path\": data_dir / \"tg-smiles-pid-polymer-class/TgSS_enriched_cleaned.csv\",\n",
    "                \"col\": \"Tg\",\n",
    "            },\n",
    "            # {\n",
    "            #     \"ex_path\": data_dir / \"smiles-extra-data/data_dnst1.xlsx\",\n",
    "            #     \"col\": \"Density\",\n",
    "            #     \"rename_d\": {\"density(g/cm3)\": \"Density\"}, \n",
    "            # },\n",
    "            {\n",
    "                \"ex_path\": data_dir / \"smiles-extra-data/data_tg3.xlsx\",\n",
    "                \"col\": \"Tg\",\n",
    "                \"rename_d\": {\"Tg [K]\": \"Tg\"}, \n",
    "            },\n",
    "            {\n",
    "                \"ex_path\": data_dir / \"smiles-extra-data/JCIM_sup_bigsmiles.csv\",\n",
    "                \"col\": \"Tg\",\n",
    "                \"rename_d\": {\"Tg (C)\": \"Tg\"}, \n",
    "            },\n",
    "        ]\n",
    "        for d in external_data_dict:\n",
    "            print(f\"ex_path: {str(d['ex_path'])}\")\n",
    "            train = add_external_data(\n",
    "                df=train,\n",
    "                ex_path=d.get(\"ex_path\"),\n",
    "                col=d.get(\"col\"),\n",
    "                rename_d=d.get(\"rename_d\"),\n",
    "                is_complement=config[\"is_complement\"]\n",
    "            )\n",
    "            print(\"after train.shape: \", train.shape)\n",
    "\n",
    "    train = add_maccs(train)\n",
    "\n",
    "    # rdkit „ÅÆË®òËø∞Â≠ê, morgan finger print\n",
    "    train = add_descriptors(train, radius=2, fp_size=1024)\n",
    "\n",
    "    new_cols = []\n",
    "    seen = {}\n",
    "    for col in train.columns:\n",
    "        if col in seen:\n",
    "            seen[col] += 1\n",
    "            new_cols.append(f\"{col}_{seen[col]}\")\n",
    "        else:\n",
    "            seen[col] = 0\n",
    "            new_cols.append(col)\n",
    "    \n",
    "    train.columns = new_cols\n",
    "    \n",
    "    # id „ÅåÊ¨†Êêç„Åó„Å¶„ÅÑ„Çã -> ËøΩÂä†„Éá„Éº„Çø\n",
    "    train[\"is_external\"] = train[\"id\"].isnull()\n",
    "    \n",
    "    # „Ç∞„É©„ÉïÁâπÂæ¥Èáè\n",
    "    train = add_graph_features(train)\n",
    "    train = add_count_atoms(train)\n",
    "    \n",
    "    train[\"id\"] = np.arange(len(train))\n",
    "    features = train.drop(targets + [\"id\", \"SMILES\"], axis=1).columns\n",
    "    for col in features:\n",
    "        if train[col].dtype == \"object\":\n",
    "            train[col] = pd.to_numeric(train[col], errors=\"coerce\")\n",
    "    useless_cols = get_useless_cols(train.drop(targets + [\"id\", \"SMILES\"], axis=1))\n",
    "    \n",
    "    train = train.drop(useless_cols, axis=1)\n",
    "    \n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    train.to_csv(output_path / \"train.csv\", index=False)\n",
    "    print(\"Saved train.csv\")\n",
    "\n",
    "features = train.drop(targets + [\"id\", \"SMILES\"], axis=1).columns\n",
    "print(len(features))\n",
    "\n",
    "oof_dfs = []\n",
    "\n",
    "loss_table_wandb = wandb.Table([\"exp\", \"model_name\", \"fold\", \"target\", \"mae\", \"mse\"])\n",
    "all_loss_tables = []\n",
    "mae_dict = {}\n",
    "all_models = {}\n",
    "\n",
    "for idx, target_col in enumerate(targets):\n",
    "    loss_tables = []\n",
    "    print(f\"\\n=== Training for target: {target_col} ===\")\n",
    "\n",
    "    df_train = train[train[target_col].notnull()].reset_index(drop=True)\n",
    "    df_train = add_scaffold_kfold(df_train, n_splits=config[\"n_splits\"])\n",
    "    X = df_train[features]\n",
    "    y = df_train[target_col]\n",
    "    oof = np.zeros(len(X))\n",
    "    \n",
    "    models = []\n",
    "\n",
    "    for fold, tr_idx, val_idx in scaffold_cv_split(df_train, n_splits=config[\"n_splits\"]):\n",
    "        loss_table = {}\n",
    "        print(f\"fold: {fold + 1}\")\n",
    "        if target_col in targets and not config[\"debug\"]:\n",
    "            X_train_pre = X.iloc[tr_idx]\n",
    "            y_train_pre = y.iloc[tr_idx]\n",
    "    \n",
    "            dtrain_pre = lgb.Dataset(X_train_pre, label=y_train_pre)\n",
    "    \n",
    "            # valid „Éá„Éº„Çø„Çí‰Ωø„Çè„Åö„Å´Â≠¶Áøí\n",
    "            pre_model = lgb.train(\n",
    "                pre_params,\n",
    "                dtrain_pre,\n",
    "            )\n",
    "    \n",
    "            # ÂØÑ‰∏éÂ∫¶„Åå 0 „Çà„ÇäÂ§ß„Åç„ÅÑÁâπÂæ¥Èáè„ÇíÂèñ„ÇäÂá∫„Åô\n",
    "            feature_importance = pre_model.feature_importance()\n",
    "            print(np.sum(feature_importance == 0) / len(feature_importance))\n",
    "            \n",
    "            feature_name = pre_model.feature_name()\n",
    "            use_features = [feature_name[idx] for idx in range(len(feature_name)) if feature_importance[idx] > 0]\n",
    "        else:\n",
    "            use_features = features\n",
    "        print(len(use_features))        \n",
    "\n",
    "        # ÁâπÂæ¥ÈáèÈÅ∏Êäû„Åó„Å¶ valid „Éá„Éº„Çø„Å®„Å®„ÇÇ„Å´Â≠¶Áøí\n",
    "        X_train, X_val = X.iloc[tr_idx][use_features], X.iloc[val_idx][use_features]\n",
    "        y_train, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n",
    "\n",
    "        if config[\"augumented_gmm\"]:\n",
    "            X_train, y_train = add_augumented_gmm(X_train, y_train)    \n",
    "        \n",
    "        dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "        dval = lgb.Dataset(X_val, label=y_val, reference=dtrain)\n",
    "\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            valid_sets=[dtrain, dval],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=50),\n",
    "                lgb.log_evaluation(200)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        save_lgb_model(model, str(model_output_path / f\"model_{target_col}_{fold}.txt\"))\n",
    "\n",
    "        pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "        oof[val_idx] = pred\n",
    "\n",
    "        mse = mean_squared_error(y_val, pred)\n",
    "        mae = mean_absolute_error(y_val, pred)\n",
    "        loss_table[\"fold\"] = fold\n",
    "        loss_table[\"target\"] = target_col\n",
    "        loss_table[\"mae\"] = mae\n",
    "        loss_table[\"mse\"] = mse\n",
    "\n",
    "        loss_tables.append(loss_table)\n",
    "        models.append(model)\n",
    "\n",
    "    score_mse = mean_squared_error(y, oof)\n",
    "    score_mae = mean_absolute_error(y, oof)\n",
    "    print(f\"RMSE for {target_col}: {score_mse:.4f}\")\n",
    "    print(f\"MAE for {target_col}: {score_mae:.4f}\")\n",
    "    mae_dict[target_col] = score_mae\n",
    "\n",
    "    for loss_table in loss_tables:\n",
    "        loss_table_wandb.add_data(exp, model_name, loss_table[\"fold\"], loss_table[\"target\"], loss_table[\"mae\"], loss_table[\"mse\"])\n",
    "    all_loss_tables += loss_tables\n",
    "\n",
    "    oof_df = pd.DataFrame({\n",
    "        \"id\": df_train[\"id\"].values,\n",
    "        target_col: oof\n",
    "    })\n",
    "    oof_dfs.append(oof_df)   \n",
    "\n",
    "    all_models[target_col] = models\n",
    "\n",
    "wandb.log({\"fold_target_losses\": loss_table_wandb})\n",
    "# target ÊØé„ÅÆ Âπ≥Âùá mae „ÇíË®òÈå≤\n",
    "for target in targets:\n",
    "    key_name = f\"{target}_mean_mae\"\n",
    "    mae_values = mae_dict[target]\n",
    "    # mae_values = [d[\"mae\"] for d in all_loss_tables if d[\"target\"] == target]\n",
    "    wandb.log({key_name: np.mean(mae_values)})\n",
    "\n",
    " \n",
    "# CV Ë®àÁÆó\n",
    "oof_df = pd.DataFrame()\n",
    "oof_df[\"id\"] = train[\"id\"]\n",
    "for i_oof in oof_dfs:\n",
    "    oof_df = oof_df.merge(i_oof, on=\"id\", how=\"left\")\n",
    "oof_df.to_csv(output_path / \"oof.csv\", index=False)\n",
    "\n",
    "solution = train[[\"id\"] + targets].copy()\n",
    "solution = solution.fillna(NULL_FOR_SUBMISSION)\n",
    "oof_df = oof_df.fillna(NULL_FOR_SUBMISSION)\n",
    "\n",
    "# Ë©ï‰æ°\n",
    "final_score = score(\n",
    "    solution=solution,\n",
    "    submission=oof_df,\n",
    ")\n",
    "print(f\"\\nüìä Final OOF Score (wMAE): {final_score:.6f}\")\n",
    "wandb.log({\"wMAE\": final_score})\n",
    "\n",
    "# target ÊØé„ÅÆ best_iteration „Çí‰øùÂ≠ò„Åô„Çã„ÄÇ‰øùÂ≠ò„Åó„Åü„É¢„Éá„É´„Å´„ÅØË®òÈå≤„Åï„Çå„Å¶„Å™„Åã„Å£„Åü\n",
    "best_iterations = {}\n",
    "for target in targets:\n",
    "    target_best_iterations = [model.best_iteration for model in all_models[target]]\n",
    "    best_iterations[target] = target_best_iterations\n",
    "print(best_iterations)\n",
    "\n",
    "with open(output_path / \"best_iterations.json\", \"w\") as f:\n",
    "    json.dump(best_iterations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd393a08-7a8a-40ac-8ad7-b11bc3a78608",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opp2025_myenv",
   "language": "python",
   "name": "opp2025_myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
